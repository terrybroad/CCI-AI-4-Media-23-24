{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPHN7PJgKOzb"
      },
      "source": [
        "# Using CLIP for image retrieval\n",
        "\n",
        "This notebook will show us how to use CLIP for image retrieval, using both text and images. \n",
        "\n",
        "First we will need to install the package `open_clip_torch`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BpdJkdBssk9",
        "outputId": "514354f7-3d50-4ef9-bbbe-88ecbc382cd9"
      },
      "outputs": [],
      "source": [
        "!pip install open_clip_torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets do some imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import open_clip\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from open_clip import tokenizer\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFxgLV5HAEEw"
      },
      "source": [
        "### Loading the model\n",
        "\n",
        "Lets load in our CLIP model that we will be using to encode text and images into a shared embedding space. We will be using the `convnext` models trained on the `laion2b` dataset. \n",
        "\n",
        "To see all available CLIP models you can print `clip.available_models()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pAHe7nD9rBa"
      },
      "outputs": [],
      "source": [
        "model, _, preprocess = open_clip.create_model_and_transforms('convnext_base_w', pretrained='laion2b_s13b_b82k_augreg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets inspect our model. This will show us the number of parameters our CLIP model has, the number of tokens our model can process in its context window and the number of tokens in the CLIP models vocabulary. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBRVTY9lbGm8",
        "outputId": "7fc2f6c9-d7c1-4fe4-fda9-dfd372d03834"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21slhZGCqANb"
      },
      "source": [
        "### Image Preprocessing\n",
        "\n",
        "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
        "\n",
        "The second return value from `clip.load()` contains a torchvision `Transform` that performs this preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6cpiIFHp9N6",
        "outputId": "84943ad9-0b1a-4200-946d-a8fb164ec0b0"
      },
      "outputs": [],
      "source": [
        "preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSB5jZki3Cj"
      },
      "source": [
        "### Text Preprocessing\n",
        "\n",
        "We use a case-insensitive tokenizer, which can be invoked using `tokenizer.tokenize()`. By default, the outputs are padded to become 77 tokens long, which is what the CLIP models expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGom156-i2kL",
        "outputId": "f4b4f532-f8ea-424f-d878-8cd014dad5e4"
      },
      "outputs": [],
      "source": [
        "tokenizer.tokenize(\"Hello World!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W8ARJVqBJXs"
      },
      "source": [
        "### Setting up input images and texts\n",
        "\n",
        "We are going to feed 8 example images and their textual descriptions to the model, and compare the similarity between the corresponding features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMc1AXzBlhzm"
      },
      "outputs": [],
      "source": [
        "# images in skimage to use and their textual descriptions\n",
        "descriptions = {\n",
        "    \"page\": \"a page of text about segmentation\",\n",
        "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
        "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
        "    \"rocket\": \"a rocket standing on a launchpad\",\n",
        "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
        "    \"camera\": \"a person looking at a camera on a tripod\",\n",
        "    \"horse\": \"a black-and-white silhouette of a horse\",\n",
        "    \"coffee\": \"a cup of coffee on a saucer\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets look at the loaded images with their corresponding descriptions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "NSSrLY185jSf",
        "outputId": "d057493e-01e1-4673-b549-0cb8a0a6b2d2"
      },
      "outputs": [],
      "source": [
        "original_images = []\n",
        "images = []\n",
        "texts = []\n",
        "filenames = []\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "im_folder_path = '../media/demo_images/'\n",
        "\n",
        "for filename in [filename for filename in os.listdir(im_folder_path) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
        "    name = os.path.splitext(filename)[0]\n",
        "    if name not in descriptions:\n",
        "        continue\n",
        "\n",
        "    image = Image.open(os.path.join(im_folder_path, filename)).convert(\"RGB\")\n",
        "\n",
        "    plt.subplot(2, 4, len(images) + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"{filename}\\n{descriptions[name]}\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    original_images.append(image)\n",
        "    images.append(preprocess(image))\n",
        "    texts.append(descriptions[name])\n",
        "    filenames.append(name)\n",
        "\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEVKsji6WOIX"
      },
      "source": [
        "### Building features\n",
        "\n",
        "We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBgCanxi8JKw"
      },
      "outputs": [],
      "source": [
        "image_input = torch.tensor(np.stack(images))\n",
        "print(image_input.shape)\n",
        "text_tokens = tokenizer.tokenize([\"This is \" + desc for desc in texts])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN9I0nIBZ_vW"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    text_features = model.encode_text(text_tokens).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuxm2Gt4Wvzt"
      },
      "source": [
        "### Calculating cosine similarity\n",
        "\n",
        "Here we are normalising the features of both the embedding tensors calculated for the images and text. We then perform a matrix multiplication between the two matricies (with [the @ operator](https://www.logilax.com/numpy-at-operator/)). Performing a matrix multiplication between these two tensors is the same as taking the dot product of all the vectors in each row+column of the two matricies (text embeddings and image embeddings). This mathematical operation is the same as [taking the cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKAxkQR7bf3A"
      },
      "outputs": [],
      "source": [
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot confusion matrix\n",
        "\n",
        "Here we will plot the confusion matrix to see how similiar our text and image embeddings are to each other:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C5zvMxh8cU6m",
        "outputId": "dc68e0ed-660b-4a65-82ff-5ae173731497"
      },
      "outputs": [],
      "source": [
        "count = len(descriptions)\n",
        "\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "plt.yticks(range(count), texts, fontsize=18)\n",
        "plt.xticks([])\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "  plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, count - 0.5])\n",
        "plt.ylim([count + 0.5, -2])\n",
        "\n",
        "plt.title(\"Cosine similarity between text and image features\", size=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Make new query with text\n",
        "\n",
        "Here we are going to use a new individual text string and compare it to our array of image features. We will then find use this information to find the image that is the closest match to our text prompt. \n",
        "\n",
        "Try changing the text prompt to how the match differs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_text = \"This is a picture of a bicycle\"\n",
        "query_tokens = tokenizer.tokenize([query_text])\n",
        "\n",
        "with torch.no_grad():\n",
        "    query_features = model.encode_text(query_tokens).float()\n",
        "\n",
        "query_features /= query_features.norm(dim=-1, keepdim=True)\n",
        "q_similarity = query_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "\n",
        "max_index = np.argmax(q_similarity)\n",
        "print(f'The closest match for \\\"{query_text}\\\" is the image: {filenames[max_index]}')\n",
        "original_images[max_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Make new query with an image\n",
        "\n",
        "As well as performing information retrieval with text, we can also do it with images. Here let's load in a sketch image and see what we can retrieve. This is exactly the method used in the [Sketchy Collections](https://ualshowcase.arts.ac.uk/project/316244/cover) project by DSAI alumni Polo Sologub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sketch_image = Image.open('../media/bike-sketch.jpg').convert(\"RGB\")\n",
        "sketch_im_np = preprocess(sketch_image).unsqueeze(0)\n",
        "sketch_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    sketch_features = model.encode_image(sketch_im_np).float()\n",
        "\n",
        "sketch_features /= sketch_features.norm(dim=-1, keepdim=True)\n",
        "s_similarity = sketch_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "\n",
        "max_index = np.argmax(q_similarity)\n",
        "print(f'The closest match for the sketch query is the image: {filenames[max_index]}')\n",
        "original_images[max_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tasks\n",
        "**Task 1:** Run through this all of the code cells in this notebook and spend time reading and understanding the code.\n",
        "\n",
        "**Task 2:** Try changing using different [text queries](#make-new-query-with-text) and [image queries](#make-new-query-with-an-image) to see what results you get. \n",
        "\n",
        "**Task 3:** Based on the code you have seen here, can you build a simple information retrieval system with a larger dataset of images, and use either text or images to query it? You will need to:\n",
        " - Load in a selection of images \n",
        " - Get embeddings for all the images with CLIP (you may need to do this in batches depending on the size of the dataset)\n",
        " - Get the embedding for the query text or image\n",
        " - Calculate the cosine similiarity between the query and the dataset\n",
        " - Display the result based on the query\n",
        "\n",
        "You can use one of the datasets you already have, such as the dataset you made from week 3, or you can download one from [kaggle](https://www.kaggle.com/) or download this [sample of the Metropolitan museum collection](http://ptak.felk.cvut.cz/met/dataset/test_met.tar.gz).\n",
        "\n",
        "### Bonus exercises\n",
        "\n",
        "Here are a few other ways you could extend and adapt CLIP for your projects:\n",
        "\n",
        "**A:** Instead of searching for images with text can you search for documents of text with images? You could use CLIP to process either [the limerick](https://git.arts.ac.uk/tbroad/limerick-dataset) or [haiku](https://git.arts.ac.uk/tbroad/haiku-dataset) datasets. \n",
        "\n",
        "**B:** Can you use CLIP to train a CPPN (from week 2) or guide the generation of a GAN (from week 6) and use it to search the latent space for a specific text prompt?\n",
        "\n",
        "**C:** Can you build a interactive application in Dorothy that uses CLIP to do image retrieval in real-time based on the web-cam image. You could hold up drawings to the webcam to make your own prototype of the [sketchy collections project](https://ualshowcase.arts.ac.uk/project/316244/cover)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Interacting with CLIP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
