{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GAN\n",
    "\n",
    "This notebook contains all of the code for training your own generative adversarial network. By default this notebooks is trained on images from the MNIST dataset (run `download-mnist-dataset.ipynb` first to download this yourself). Once you have trained it on MNIST you can try training it on another dataset of your choosing later on. \n",
    "\n",
    "This implementation is based on the DCGAN paper from 2015: https://arxiv.org/abs/1511.06434\n",
    "\n",
    "DCGAN is a simple implementation that is easy to understand. However they require a lot of data (10,000+ images) and they only work well when the data is not too varied and images are aligned i some way. For training a higher fidelty GAN on a smaller dataset, see this notebook for fine-tuning styleGAN3: https://colab.research.google.com/github/dvschultz/stylegan3/blob/main/SG3.ipynb\n",
    "\n",
    "Work your way through the code here. You can have a go at running this code and training your own model, however it is unlikely that you will be able to train your own model on a laptop CPU in the relatively short time we have in class.\n",
    "\n",
    "Given the resource restrictions we have, there is a pre-trained model included in this weeks code `gan_weights_mnist_1000_epochs.pt` that you can use to do some tasks with generation in `gan-generation-tasks.ipynb` and `interactive-gan-generation.py`. Once you have worked through this notebook you can move onto those notebooks in your own time.\n",
    "\n",
    "The code in this notebook is heavily modified from: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision \n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from IPython.display import HTML\n",
    "from src.util import weights_init\n",
    "from src.gan_model import Generator, Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and model hyper-parameters\n",
    "device = 'cpu'\n",
    "num_epochs = 20 # Number of epochs in training\n",
    "batch_size = 128 # Number images in a training batch\n",
    "image_size = 32 # Image resolution for training\n",
    "num_channels = 3 # Number of channels in the iamge\n",
    "z_dim = 100 # Size of latent dimension\n",
    "n_f_maps = 32 # Number of convolutional feature maps in model\n",
    "learning_rate = 0.0002 # Learning rate for optimiser\n",
    "beta1 = 0.5 # Hyperparameter for ADAM optimiser\n",
    "\n",
    "# Path to dataset\n",
    "data_path = '../data/class-datasets/mnist'\n",
    "\n",
    "# Set to true to start from existing trained weights\n",
    "load_chk = False\n",
    "save_path = 'gan_weights.pt'\n",
    "load_path = 'gan_weights.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = torchvision.datasets.ImageFolder(root=data_path,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "    \n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup core objects\n",
    "\n",
    "Navigate to `src/gan_model.py` and spend some time reading the code there to see how the Generator and Discriminator are implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(n_f_maps,num_channels).to(device)\n",
    "generator = Generator(z_dim,n_f_maps,num_channels).to(device)\n",
    "\n",
    "if load_chk == True:\n",
    "    checkpoint_sd = torch.load(load_path, map_location=torch.device('cpu'))\n",
    "    generator.load_state_dict(checkpoint_sd['generator'])\n",
    "    discriminator.load_state_dict(checkpoint_sd['discriminator'])\n",
    "else:\n",
    "    # Initial wieghts with values in specific range - see src/util.py for the implementation\n",
    "    discriminator.apply(weights_init)\n",
    "    generator.apply(weights_init)\n",
    "\n",
    "# Our loss function is the Binary Cross Entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, z_dim, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=learning_rate, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Cycle through batches of the dataset \n",
    "    for i, data_batch in enumerate(dataloader, 0):\n",
    "        \n",
    "        # Our dataset loader will give us the image batch and the associated class label \n",
    "        # We can ignore the class labels as we are treating the images in all folders as one big dataset\n",
    "        image_data, class_label = data_batch\n",
    "        image_data = image_data.to(device)\n",
    "        # -----------------------------------------------------------\n",
    "        # Phase 1: Update Discriminator network\n",
    "        # Here we do two passes before updating the weights\n",
    "        \n",
    "        # Clear gradients of discriminator\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # Train with a batch of real images\n",
    "        \n",
    "        minibatch_size = image_data.size(0)\n",
    "        label = torch.full((minibatch_size,), real_label, dtype=torch.float, device=device)\n",
    "        pred_D = discriminator(image_data).squeeze()\n",
    "        errD_real = criterion(pred_D, label)\n",
    "        errD_real.backward()\n",
    "        D_x = pred_D.mean().item()\n",
    "\n",
    "        ## Train with a batch of fake images\n",
    "        noise = torch.randn(minibatch_size, z_dim, 1, 1, device=device)\n",
    "        fake = generator(noise)\n",
    "        label.fill_(fake_label)\n",
    "        pred_D = discriminator(fake.detach()).squeeze()\n",
    "        errD_fake = criterion(pred_D, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = pred_D.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        \n",
    "        # Update the weights of the discriminator network only\n",
    "        optimizerD.step()\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Phase 2: Update Generator network\n",
    "        # Here we do two passes before updating the weights\n",
    "\n",
    "        # Clear gradients of discriminator\n",
    "        generator.zero_grad()\n",
    "\n",
    "        # Take fake images and pass trh\n",
    "        noise = torch.randn(minibatch_size, z_dim, 1, 1, device=device)\n",
    "        fake = generator(noise)\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        pred_D = discriminator(fake).squeeze()\n",
    "        errG = criterion(pred_D, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = pred_D.mean().item()\n",
    "        \n",
    "        # Update generator weights only\n",
    "        optimizerG.step()\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Now lets track our progress\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x) mean: %.4f\\tD(G(z)) mean: %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = generator(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            \n",
    "            # Save model weights\n",
    "            models_dict = {}\n",
    "            models_dict['generator'] = generator.state_dict()\n",
    "            models_dict['discriminator'] = discriminator.state_dict()\n",
    "            torch.save(models_dict, save_path)\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot lossses over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interactive visualisation of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare generated and real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "\n",
    "**Task 1:** Run this notebook to see how GAN training operates. You probably only want to do a small number of epochs (between 3 and 10) to see how this goes. While this is training, move on to **Task 2**. Once you are done you can always interrupt [the training loop cell](#training-loop) and run the cells after to plot the performance of your model.\n",
    "\n",
    "**Task 2:** Look at the implementation code for the **Generator** and **Discriminator** models in`src/gan_model.py` can you write some comments here to describe the differences between the models and their layer types?\n",
    "\n",
    "**Task 3:** Move on to `gan-generation-tasks.ipynb` and do the tasks described there. You can either use your own trained model or the pre-trained model (`gan_weights_mnist_1000_epochs.pt) included in this weeks code notebook. \n",
    "\n",
    "**Task 4:** Move on to running the `interactive-gan-generation.py` script. Can you extend it to change the interaction that controls training? Such as using YOLO to create a controller to interact with the latent space or [use audio features](https://github.com/Louismac/dorothy/blob/main/examples/fft.py) to manipulate the latent variables before generation?\n",
    "\n",
    "**Bonus task:** Following the material from the lecture, [take a look at this notebook for training an autoencoder and a variational autoencoder](https://avandekleut.github.io/vae/), what are the differences to this notebook when it comes to the implementation of the training? Download the notebook and run it for yourself. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
