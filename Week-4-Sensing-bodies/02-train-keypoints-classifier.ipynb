{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Keypoints Classifier, i.e. Pose Classifier\n",
    "\n",
    "In this notebook we are going to train our own pose classifier in PyTorch based on the dataset we built from the python script and notebook `00` and `01` respectively.\n",
    "\n",
    "**Before you go any further** make sure you have already created and saved your csv file in `../data/my-data`.\n",
    "\n",
    "Code adapted from this [repo](https://github.com/Alimustoofaa/YoloV8-Pose-Keypoint-Classification/tree/master).\n",
    "\n",
    "First let's do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are getting an error you may need to uncomment the next line to install sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameters\n",
    "\n",
    "Now let's define our hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "num_epochs = 200\n",
    "num_classes = 3\n",
    "test_size = 0.3\n",
    "batch_size = 128\n",
    "learn_rate = 0.001\n",
    "data_path = '../data/my-data/poses_keypoints.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read Dataset\n",
    "\n",
    "Here, we are reading the first 5 rows of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "df = df.drop('image_name', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count and plot our data per class\n",
    "\n",
    "In the following two cells, we are counting and plotting the number of data per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts().plot(kind=\"bar\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the 1st column as our labels `y` and the following 34 columns as our keypoints input dataset `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the encoder label, to turn each label into an index number\n",
    "encoder = LabelEncoder()\n",
    "y_label = df['label']\n",
    "y = encoder.fit_transform(y_label)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keypoint dataset \n",
    "X = df.iloc[:, 1:] # start from 11: if you want to skip the keipoints of the face\n",
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Test Split\n",
    "\n",
    "Perform a train-test split with test_size=0.3 (defined in our hyperparameters), and a random but deterministic split and a strification.\n",
    "\n",
    "Stratified sampling is a method of sampling that involves dividing a population into homogeneous subgroups known as strata, and then sampling from each stratum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Number of Training keypoints: \", len(X_train))\n",
    "print(\"Number of Testing keypoints: \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional step if you want to explore how the stratification creates homogenous subsets of data.\n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "# print(Counter(y))\n",
    "# print(Counter(y_train))\n",
    "# print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A glimpse into the test data in a table format\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MinMax scaling to scale each feature into a given range\n",
    "\n",
    "For more information, look [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A glipse into the test data in the format of an array and after performing a minmax scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Loader\n",
    "\n",
    "The data are currently numpy arrays and need to get transformed into torch tensors in order to get into the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataKeypointClassification(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.x = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.int64))\n",
    "        self.n_samples = X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataKeypointClassification(X_train, y_train)\n",
    "test_dataset = DataKeypointClassification(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define our simple feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(NeuralNet, self).__init__()\n",
    "      self.fc1 = nn.Linear(X_train.shape[1], 256)\n",
    "      self.fc2 = nn.Linear(256, num_classes)     \n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup core objects\n",
    "\n",
    "Here we setup our core objects, the model, the loss function and the optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet()\n",
    "model.to(device)\n",
    "\n",
    "# Cross entropy loss for training classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimiser\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training loop\n",
    "\n",
    "Here is our training loop for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "best_loss = 100000\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Get data\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Process data\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Update model weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Normalise cumulative losses to dataset size\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Added cumulative losses to lists for later display\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, train loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Train loss\")\n",
    "plt.plot(train_losses,label=\"train\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"cumulative loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test our model\n",
    "\n",
    "Here we use the model to predict the label on unseen data, our test data.\n",
    "\n",
    "The predictions are the predicted classes (in the encoded format 0-1-2) for each item in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = torch.from_numpy(X_test.astype(np.float32))\n",
    "test_labels = y_test\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_features)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a really good way to visualise the number of true positives, false negatives, false positives, and true negatives.\n",
    "\n",
    "The header row corresponds to the predicted labels while the first column corresponds to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, predictions)\n",
    "df_cm = pd.DataFrame(\n",
    "    cm, \n",
    "    index = encoder.classes_,\n",
    "    columns = encoder.classes_\n",
    ")\n",
    "df_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualising the confusion matrix with a seaborn heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.ylabel(\"Surface Ground Truth\")\n",
    "    plt.xlabel(\"Predicted Surface\")\n",
    "    plt.legend()\n",
    "    \n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_SAVE = './pose_classifier.pt'\n",
    "torch.save(model.state_dict(), PATH_SAVE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference =  NeuralNet()\n",
    "model_inference.load_state_dict(torch.load(PATH_SAVE, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature, label = test_dataset.__getitem__(12) # test out different item numbers\n",
    "\n",
    "out = model_inference(feature)\n",
    "_, predict = torch.max(out, -1)\n",
    "print(f'\\\n",
    "    prediction label : {encoder.classes_[predict]} \\n\\\n",
    "    ground thrut label : {encoder.classes_[label]}'\n",
    "    )\n",
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks:\n",
    "\n",
    "**Task 1:** Run all the cells in this code to train your own pose classifier.\n",
    "\n",
    "**Task 2:** Test out the training with different hyperparameters.\n",
    "\n",
    "**Bonus Task 1:**\n",
    "\n",
    "If you created more than one datasets from the previous notebook, test the same network architecture on your different training datasets and observe how the their size and the variations within their classes might affect the loss and the confusion matrices at the end of the training.\n",
    "\n",
    "**Bonus Task 2:** \n",
    "\n",
    "You could perform an extra train-test split to create a validation subset and calculate the validation losses along with the train losses during the training, similar to what we did last week. Look into week 3 `train-image-classifier-from-scratch` for reference. \n",
    "\n",
    "To perform a second split, you will first need to split your input data into training and temp subsets and then, the temp subset into test and validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
