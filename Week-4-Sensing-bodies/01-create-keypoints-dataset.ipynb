{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Keypoints Dataset\n",
    "\n",
    "In this notebook we are going to make our own pose classification dataset in a csv format, which we can then use to train a pose classifier.\n",
    "\n",
    "The csv file will be created by extracting the keypoints (x, y coordinates of every body landmark) from images that contain human body poses. \n",
    "\n",
    "Therefore, the primary material we need to work with, is images. There are many ways to collect images for that cause:\n",
    "\n",
    "- Use the `00-collect-images-from-webcam` python script\n",
    "\n",
    "- Download public image datasets from Kaggle, eg. [yoga pose image dataset](https://www.kaggle.com/datasets/ujjwalchowdhury/yoga-pose-classification) and organise them in the respective folder\n",
    "\n",
    "- Record videos of yourself or others (as long as you have their consent) performing specific poses each time, and then use `ffmpeg` to extract the frames for each class. This process will return folders of frames, similar to what you get from option 1. Some quick instructions for `ffmpeg`:\n",
    "\n",
    "Download it from [here](https://ffmpeg.org/download.html)\n",
    "Then, open the terminal, move into the directory (cd PATH) where you have the video you want to transform into frames, create a folder to save your frames in it and run the command:\n",
    "\n",
    "`ffmpeg -i file_name.mov -r 1 -s WxH -f image2 folder_name/%03d.jpeg`\n",
    "\n",
    "This will extract one video frame per second from the video and will output them in files named 001.jpeg, 002.jpeg, etc. Images will be rescaled to fit the new WxH values (you can skipp -s WxH if you do not wish to resize your images). Look [here](https://ffmpeg.org/ffmpeg.html) for more information on `ffmpeg`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call and Test Yolov8-Pose to Extract Keypoints from Image\n",
    "\n",
    "Code adapted from this [repo](https://github.com/Alimustoofaa/YoloV8-Pose-Keypoint-Classification/tree/master).\n",
    "\n",
    "First let's do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pydantic import BaseModel\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell returns an error, uncomment the next line and run it to install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download and save the pre-trained model YOLO for pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(model='yolov8m-pose.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List out the names of our classes/labels in your dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might want to rename your folders with more meaningful names if they are currently named with numbers\n",
    "# You can do that manually and then re-run this cell to see all your names listed\n",
    "!ls ../data/my-data/my-pose-classification-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select a sample image from our dataset and apply the keypoint predictions from YOLO on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('../data/my-data/my-pose-classification-dataset/folder_name/sample_file_name.jpg')\n",
    "result = model.predict(image, save=False)[0]\n",
    "result.keypoints[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualise the predictions of YOLO on our sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(cv2.cvtColor(result.plot(), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset of Keypoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keypoint extract\n",
    "\n",
    "YOLOv8 pose estimation returns 17 keypoints:\n",
    "* NOSE:           int = 0\n",
    "* LEFT_EYE:       int = 1\n",
    "* RIGHT_EYE:      int = 2\n",
    "* LEFT_EAR:       int = 3\n",
    "* RIGHT_EAR:      int = 4\n",
    "* LEFT_SHOULDER:  int = 5\n",
    "* RIGHT_SHOULDER: int = 6\n",
    "* LEFT_ELBOW:     int = 7\n",
    "* RIGHT_ELBOW:    int = 8\n",
    "* LEFT_WRIST:     int = 9\n",
    "* RIGHT_WRIST:    int = 10\n",
    "* LEFT_HIP:       int = 11\n",
    "* RIGHT_HIP:      int = 12\n",
    "* LEFT_KNEE:      int = 13\n",
    "* RIGHT_KNEE:     int = 14\n",
    "* LEFT_ANKLE:     int = 15\n",
    "* RIGHT_ANKLE:    int = 16\n",
    "\n",
    "Since YOLO does not return the keypoints with their respective labels, we will create the following class for easy access to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetKeypoint(BaseModel):\n",
    "    NOSE:           int = 0\n",
    "    LEFT_EYE:       int = 1\n",
    "    RIGHT_EYE:      int = 2\n",
    "    LEFT_EAR:       int = 3\n",
    "    RIGHT_EAR:      int = 4\n",
    "    LEFT_SHOULDER:  int = 5\n",
    "    RIGHT_SHOULDER: int = 6\n",
    "    LEFT_ELBOW:     int = 7\n",
    "    RIGHT_ELBOW:    int = 8\n",
    "    LEFT_WRIST:     int = 9\n",
    "    RIGHT_WRIST:    int = 10\n",
    "    LEFT_HIP:       int = 11\n",
    "    RIGHT_HIP:      int = 12\n",
    "    LEFT_KNEE:      int = 13\n",
    "    RIGHT_KNEE:     int = 14\n",
    "    LEFT_ANKLE:     int = 15\n",
    "    RIGHT_ANKLE:    int = 16\n",
    "\n",
    "get_keypoint = GetKeypoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a function for extracting keypoints while using the class above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoint(keypoint):\n",
    "    # nose\n",
    "    nose_x, nose_y = keypoint[get_keypoint.NOSE]\n",
    "    # eye\n",
    "    left_eye_x, left_eye_y = keypoint[get_keypoint.LEFT_EYE]\n",
    "    right_eye_x, right_eye_y = keypoint[get_keypoint.RIGHT_EYE]\n",
    "    # ear\n",
    "    left_ear_x, left_ear_y = keypoint[get_keypoint.LEFT_EAR]\n",
    "    right_ear_x, right_ear_y = keypoint[get_keypoint.RIGHT_EAR]\n",
    "    # shoulder\n",
    "    left_shoulder_x, left_shoulder_y = keypoint[get_keypoint.LEFT_SHOULDER]\n",
    "    right_shoulder_x, right_shoulder_y = keypoint[get_keypoint.RIGHT_SHOULDER]\n",
    "    # elbow\n",
    "    left_elbow_x, left_elbow_y = keypoint[get_keypoint.LEFT_ELBOW]\n",
    "    right_elbow_x, right_elbow_y = keypoint[get_keypoint.RIGHT_ELBOW]\n",
    "    # wrist\n",
    "    left_wrist_x, left_wrist_y = keypoint[get_keypoint.LEFT_WRIST]\n",
    "    right_wrist_x, right_wrist_y = keypoint[get_keypoint.RIGHT_WRIST]\n",
    "    # hip\n",
    "    left_hip_x, left_hip_y = keypoint[get_keypoint.LEFT_HIP]\n",
    "    right_hip_x, right_hip_y = keypoint[get_keypoint.RIGHT_HIP]\n",
    "    # knee\n",
    "    left_knee_x, left_knee_y = keypoint[get_keypoint.LEFT_KNEE]\n",
    "    right_knee_x, right_knee_y = keypoint[get_keypoint.RIGHT_KNEE]\n",
    "    # ankle\n",
    "    left_ankle_x, left_ankle_y = keypoint[get_keypoint.LEFT_ANKLE]\n",
    "    right_ankle_x, right_ankle_y = keypoint[get_keypoint.RIGHT_ANKLE]\n",
    "    \n",
    "    return [\n",
    "        nose_x, nose_y,\n",
    "        left_eye_x, left_eye_y,\n",
    "        right_eye_x, right_eye_y,\n",
    "        left_ear_x, left_ear_y,\n",
    "        right_ear_x, right_ear_y,\n",
    "        left_shoulder_x, left_shoulder_y,\n",
    "        right_shoulder_x, right_shoulder_y,\n",
    "        left_elbow_x, left_elbow_y,\n",
    "        right_elbow_x, right_elbow_y,\n",
    "        left_wrist_x, left_wrist_y,\n",
    "        right_wrist_x, right_wrist_y,\n",
    "        left_hip_x, left_hip_y,\n",
    "        right_hip_x, right_hip_y,\n",
    "        left_knee_x, left_knee_y,\n",
    "        right_knee_x, right_knee_y,        \n",
    "        left_ankle_x, left_ankle_y,\n",
    "        right_ankle_x, right_ankle_y\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create our list of labels/classes based on the folders' names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = '../data/my-data/my-pose-classification-dataset'\n",
    "pose_list = sorted((f for f in os.listdir(dataset_root) if not f.startswith(\".\")), key=str.lower) # to avoid included hidden files like .DS_Store\n",
    "pose_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run through all the images of our dataset and extract the keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_csv = []\n",
    "for pose in pose_list:\n",
    "\n",
    "    image_path_list = glob.glob(f'{dataset_root}/{pose}/*.jpg')\n",
    "    for image_path in image_path_list:\n",
    "        # get image_name\n",
    "        image_name = image_path.split('/')[-1]\n",
    "        # read numpy image\n",
    "        image = cv2.imread(image_path)\n",
    "        # get height width image\n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        # detect pose using yolov8-pose\n",
    "        results = model.predict(image, save=False)[0]\n",
    "        # get the normalised values of x and y\n",
    "        results_keypoint = results.keypoints.xyn.numpy()\n",
    "\n",
    "        for result_keypoint in results_keypoint:\n",
    "            if len(result_keypoint) == 17:\n",
    "                keypoint_list = extract_keypoint(result_keypoint)\n",
    "                # insert image_name, labe] in index 0,1 for the first 2 columns of the csv file\n",
    "                keypoint_list.insert(0, image_name)\n",
    "                keypoint_list.insert(1, pose)\n",
    "                dataset_csv.append(keypoint_list)\n",
    "        # break\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the dataset_csv list in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv\n",
    "header = [\n",
    "    'image_name',\n",
    "    'label',\n",
    "    # nose\n",
    "    'nose_x',\n",
    "    'nose_y',\n",
    "    # left eye\n",
    "    'left_eye_x',\n",
    "    'left_eye_y',\n",
    "    # right eye\n",
    "    'right_eye_x',\n",
    "    'right_eye_y',\n",
    "    # left ear\n",
    "    'left_ear_x',\n",
    "    'left_ear_y',\n",
    "    # right ear\n",
    "    'right_ear_x',\n",
    "    'right_ear_y',\n",
    "    # left shoulder\n",
    "    'left_shoulder_x',\n",
    "    'left_shoulder_y',\n",
    "    # right sholder\n",
    "    'right_shoulder_x',\n",
    "    'right_shoulder_y',\n",
    "    # left elbow\n",
    "    'left_elbow_x',\n",
    "    'left_elbow_y',\n",
    "    # rigth elbow\n",
    "    'right_elbow_x',\n",
    "    'right_elbow_y',\n",
    "    # left wrist\n",
    "    'left_wrist_x',\n",
    "    'left_wrist_y',\n",
    "    # right wrist\n",
    "    'right_wrist_x',\n",
    "    'right_wrist_y',\n",
    "    # left hip\n",
    "    'left_hip_x',\n",
    "    'left_hip_y',\n",
    "    # right hip\n",
    "    'right_hip_x',\n",
    "    'right_hip_y',\n",
    "    # left knee\n",
    "    'left_knee_x',\n",
    "    'left_knee_y',\n",
    "    # right knee\n",
    "    'right_knee_x',\n",
    "    'right_knee_y',\n",
    "    # left ankle\n",
    "    'left_ankle_x',\n",
    "    'left_ankle_y',\n",
    "    # right ankle\n",
    "    'right_ankle_x',\n",
    "    'right_ankle_y'\n",
    "]\n",
    "\n",
    "with open('../data/my-data/poses_keypoints.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # write multiple rows\n",
    "    writer.writerows(dataset_csv)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in our data/my-data folder, we should have our dataset of keypoints in a csv format, with one column with our labels, one (optional) column with the image_name and 34 columns with the 17 x 2 keypoints.\n",
    "\n",
    "##### Get the first 5 rows of the csv file we just saved (without the image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/my-data/poses_keypoints.csv')\n",
    "df = df.drop('image_name', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks:\n",
    "\n",
    "**Task 1:** Run all the cells in this code to create your own dataset. Create a dataset with at least 3 classes, with at least 200 instances in each class.\n",
    "\n",
    "**Task 2:** Visualise the YOLO predictions for different samples from your dataset, to test how it works on different poses.\n",
    "\n",
    "**Bonus Task:** \n",
    "\n",
    "If you created the initial image datasets based on the provided `00-collect-images-from-webcam` python script, you can also try recording small clips with your phone and then use `ffmpeg` on them to extract frames and organise them in the respective folders in your data. Then use this notebook again to create a different csv file for your classifier.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
