{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Week 6a: Audio classificiation\n",
        "\n",
        "This notebook performs audio classification in pytorch, using the SpeechCommands dataset: https://arxiv.org/abs/1804.03209\n",
        "\n",
        "This code implements a 1-Dimensional Convoluitonal Neural Network that classifies raw waveforms of people speaking different voice instructions. The CNN model implemented here is based on this paper: https://arxiv.org/pdf/1610.00087.pdf\n",
        "\n",
        "Before running this code you will need to download the dataset using the `download-speech-commands-dataset.ipynb` notebook. \n",
        "\n",
        "Work through this notebook and spend time reading the code. Try comparing this code side-by-side with the `train-image-classifier-from-scratch.ipynb` notebook from Week 3 to see the similiarities and differences between the two codebases. Once you have sucessfully trained a model with this code, try some of the tasks at the bottom of the notebook. \n",
        "\n",
        "The code in this notebook is heavily modified (for readability and adaptability) from this soruce: https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torchaudio.transforms as transforms\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import data util functions from 'src/'\n",
        "from src.audio_folder_dataset import AudioFolder\n",
        "from src.audio_folder_collate_fn import collate_audio_folder_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define hyperparameters\n",
        "\n",
        "If you have a Mac M1/M2 you can change the device to `mps`, if you have an NVIDIA GPU you can change the device to `cuda`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "original_sample_rate = 16000 # Sample rate for the speech commands dataset\n",
        "new_sample_rate = 8000 # Sample rate for the speech commands dataset\n",
        "val_size = 0.3 # Size of train / validation split\n",
        "batch_size = 200 # Batch size for training\n",
        "num_epochs = 2 # Number of epochs for training (this is a large dataset so not many epochs needed)\n",
        "log_interval = 20 # Log process every n interations\n",
        "learning_rate = 0.01 # Learning rate for training\n",
        "weight_decay = 0.0001 # Weight decay for ADAM optimiser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define data transform\n",
        "\n",
        "When working with torchaudio we can only define one transform here. This function downsamples the audio waveform from a sample rate of 16000 to 8000, which is fine for working with human voices and helps us train more efficiently.\n",
        "\n",
        "Unlike when working with images, the padding and normalising of the data to the same length happens in the function `collate_audio_folder_batch` in `src/audio_folder_collate_fn.py`. As all of our audio files are different lengths, we need to harmonise them when we load in a random mini-batch. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Resample(orig_freq=original_sample_rate, new_freq=new_sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create datasets\n",
        "\n",
        "Here we create our dataset classes. Because we are using different transforms, we need to make two seperate dataset classes. We will then take a random sub-selection of our data and split our dataset into two. \n",
        "\n",
        "When we do the split, by setting `random_state=42`, we are doing this in a deterministic way, such that we will always get the same 'random' split of data into the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = '../data/class-datasets/speech-commands/train/'\n",
        "\n",
        "dataset = AudioFolder(train_path, transform=transform)\n",
        "num_classes = len(dataset.classes)\n",
        "\n",
        "# Get length of dataset and indicies\n",
        "num_train = len(dataset)\n",
        "indices = list(range(num_train))\n",
        "\n",
        "# Get train / val split for data points\n",
        "train_indices, val_indices = train_test_split(indices, test_size=val_size, random_state=42)\n",
        "\n",
        "# Override dataset classes to only be samples for each split\n",
        "train_subset = torch.utils.data.Subset(dataset, train_indices)\n",
        "val_subset = torch.utils.data.Subset(dataset, val_indices)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, collate_fn=collate_audio_folder_batch, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=batch_size, collate_fn=collate_audio_folder_batch,  shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot a sample of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_batch, label_batch = next(iter(train_loader))\n",
        "sample_waveform = data_batch[0].squeeze()\n",
        "print(f'Data batch shape: {data_batch.shape}')\n",
        "print(f\"Shape of waveform: {sample_waveform.size()}\")\n",
        "sample_class = int(label_batch[0].item())\n",
        "print(f'Class of waveform: \\'{dataset.idx_to_class[sample_class]}\\'')\n",
        "plt.plot(sample_waveform.t().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Network\n",
        "\n",
        "Here we define a 1-Dimensional convolutional neural network to process raw audio data. The specific architecture is modeled after the M5 network architecture described in [this paper](https://arxiv.org/pdf/1610.00087.pdf). \n",
        "\n",
        "What are the main differences between this and the 2-D convolutional network that we used in Week 3? \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class M5(nn.Module):\n",
        "    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
        "        self.pool1 = nn.MaxPool1d(4)\n",
        "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
        "        self.pool2 = nn.MaxPool1d(4)\n",
        "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool3 = nn.MaxPool1d(4)\n",
        "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool4 = nn.MaxPool1d(4)\n",
        "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(self.bn1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(self.bn2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(self.bn3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(self.bn4(x))\n",
        "        x = self.pool4(x)\n",
        "        x = F.avg_pool1d(x, x.shape[-1])\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup core objects\n",
        "\n",
        "Here we setup our core objects, the model, the loss function (criterion) and the optimiser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = M5(n_input=1, n_output=num_classes)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n = count_parameters(model)\n",
        "print(\"Number of parameters: %s\" % n)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation functions\n",
        "\n",
        "These are functions we can use to get the actual prediction from the model in validation to get an overall accuracy score on the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def number_of_correct(pred, target):\n",
        "    # count number of correct predictions\n",
        "    return pred.squeeze().eq(target).sum().item()\n",
        "\n",
        "\n",
        "def get_likely_index(tensor):\n",
        "    # find most likely label index for each element in the batch\n",
        "    return tensor.argmax(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop\n",
        "\n",
        "\n",
        "Here is our training loop for our data. Look at how the training set and validation set are used differently. \n",
        "\n",
        "What differences are there in the code when we cycle through each of these sets of data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pbar_update = 1 / (len(train_loader) + len(val_loader))\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "best_loss = 100000\n",
        "\n",
        "# Show progress bar \n",
        "with tqdm(total=num_epochs) as pbar:\n",
        "    \n",
        "    # For each cycle of the dataset\n",
        "    for epoch in range(num_epochs):\n",
        "        # Variables to keep track of running loss\n",
        "        train_loss = 0.0\n",
        "        val_loss = 0.0\n",
        "        \n",
        "        # Put model in training model\n",
        "        model.train()\n",
        "        model.to(device)\n",
        "        \n",
        "        # Train loop\n",
        "        # For each batch in one cycle of the training set\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            pass\n",
        "            # Move data to whatever device we are running training on\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            \n",
        "            # Forward pass with the model\n",
        "            output = model(data)\n",
        "\n",
        "            # Evaluate classification accuracy\n",
        "            loss = criterion(output.squeeze(), target)\n",
        "            \n",
        "            # Backpropagate loss and update gradients\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Keep track off loss over time\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # print training stats\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
        "\n",
        "            # update progress bar\n",
        "            pbar.update(pbar_update)\n",
        "\n",
        "        # Put model in evaluation mode (turn off batch norm)\n",
        "        model.eval()\n",
        "        model.to(device)\n",
        "        # Without gradient tracking \n",
        "        with torch.no_grad():\n",
        "            \n",
        "            # Variable to track total correct classifications\n",
        "            correct = 0\n",
        "\n",
        "            # Validation loop\n",
        "            # For each batch in one cycle of the validation set\n",
        "            for data, target in val_loader:\n",
        "                \n",
        "                # Move data to whatever device we are running training on\n",
        "                data = data.to(device)\n",
        "                target = target.to(device)\n",
        "\n",
        "                # Forward pass with the model\n",
        "                output = model(data)\n",
        "\n",
        "                # Evaluate classification accuracy\n",
        "                loss = criterion(output.squeeze(), target)\n",
        "                \n",
        "                # Track loss\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Get top prediction\n",
        "                pred = get_likely_index(output)\n",
        "                \n",
        "                # Check if prediction is correct\n",
        "                correct += number_of_correct(pred, target)\n",
        "\n",
        "                # update progress bar\n",
        "                pbar.update(pbar_update)\n",
        "        \n",
        "        # Normalise cumulative losses to dataset size\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        \n",
        "        # Added cumulative losses to lists for later display\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(val_loader.dataset)} ({100. * correct / len(val_loader.dataset):.0f}%)\\n\")\n",
        "\n",
        "        # if validation score is lowest so far, save the model\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            torch.save(model.cpu().state_dict(), 'best_audio_classifier.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Train vs validation loss\")\n",
        "plt.plot(train_losses,label=\"train\")\n",
        "plt.plot(val_losses,label=\"val\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"culuative loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task\n",
        "\n",
        "**Task 1:**: Run all the code to train a model. While that is running look at this code and compare it side-by-side with the image classifier training code from Week 3. What are the main differences? What code is unchanged between the two approaches?\n",
        "\n",
        "**Task 2:** Based on the code in this notebook, create a new dataset class and data loader to load in the test dataset (see `speech-commands/test`) and get a cumulative score for the accuracy, just like what has been done with the validation set. What is the score for the test set?\n",
        "\n",
        "### Bonus tasks\n",
        "\n",
        "Here are some bonus tasks if you want to take this work further. You do not need to do these in order:\n",
        "\n",
        "**Task A:** Can you use this code to train a model on a different audio classificaiton dataset?\n",
        "\n",
        "**Task B:** Can you take your trained model and build it into an interactive application that responds to voice commands? The words used for training are the names of the folders in the training set.\n",
        "\n",
        "**Task C:** Can you change [the transform to be calculate an MFCC](https://pytorch.org/audio/main/generated/torchaudio.transforms.MFCC.html) (Mel-Frequence Cepstrum Coefficient) instead of training the network on a raw waveform? There are a couple of ways of doing this:\n",
        "- **Approach A:** [Change the downsampling transform](https://pytorch.org/audio/main/generated/torchaudio.transforms.MFCC.html) to calculate an MFCC instead. Then in the collate function (`src/audio_folder_collate_fn.py`) you can flatten the 2D MFCC respresentation into a 1-D vector before all the vectors then get padded to the same length. This way you can continue to use the 1-D CNN in this notebook. \n",
        "- **Approach B:** Keep the downsampling transform the same. Instead, calculate the MFCC in the collate function after padding has been applied (`src/audio_folder_collate_fn.py`), this way all the MFCCs will have the same dimensionality. Instead of flattening the MFCC matricies into vectors, you can then repace the [1D CNN code](#define-the-network) with a 2-D CNN instead and train a classifier on the MFCC matricies (You can borrow coe from Week 3 for this).\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
