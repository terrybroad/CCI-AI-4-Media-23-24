{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "\n",
    "In this notebook we are going to prepare our dataset into a binary which is a list of our pre-calculated tokens. Doing this step makes training more efficient as we are not doing that conversion at each training step.\n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from nanoGPT.data import prepare_dataset_as_chars, prepare_dataset_gpt_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure dataset paths\n",
    "\n",
    "By default we are going to load the complete works of shakespeare here, but you can load in whatever text file you want here as your training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = '../media/texts/shakespeare.txt'\n",
    "dataset_path = '../data/class-datasets/text-datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset as characters\n",
    "\n",
    "Here we will tokenise the dataset as characters. This is for training simple models like babyGPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../media/shakespeare.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tbroad/teaching/repos/23-24/AI-4-Media-23-24/Week-7-Transformers/00-prepare-dataset.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tbroad/teaching/repos/23-24/AI-4-Media-23-24/Week-7-Transformers/00-prepare-dataset.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prepare_dataset_as_chars(text_path, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(dataset_path, \u001b[39m'\u001b[39;49m\u001b[39mshakespeare_chars\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[0;32m~/teaching/repos/23-24/AI-4-Media-23-24/Week-7-Transformers/nanoGPT/data.py:38\u001b[0m, in \u001b[0;36mprepare_dataset_as_chars\u001b[0;34m(input_file_path, dataset_path)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_dataset_as_chars\u001b[39m(input_file_path, dataset_path):\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(input_file_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     39\u001b[0m         data \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m     40\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlength of dataset in characters: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m:\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../media/shakespeare.txt'"
     ]
    }
   ],
   "source": [
    "prepare_dataset_as_chars(text_path, os.path.join(dataset_path, 'shakespeare_chars'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset as GPT2 Tokens\n",
    "\n",
    "Here we will tokenise the dataset using OpenAIs tokeniser for GPT2. This is for fine-tuning pre-trained GPT2 models. If you are interested in how they come about these tokenizers, see [Andrej Karpathy's minibpe repo](https://github.com/karpathy/minbpe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset_gpt_tokenizer(text_path, os.path.join(dataset_path, 'shakespeare_tokens'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
