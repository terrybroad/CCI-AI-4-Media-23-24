{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2: Training neural networks\n",
    "\n",
    "In this notebook we are going to look in more depth at the process of training neural networks, with some nice visual representations where we can see our networks learning over time. We are going to try editing hyperparameters for training, such as the **learning rate** and **momentum**, and using different kinds of optimisation algorithm and eventually editing neural networks ourselves. \n",
    "\n",
    "The type of neural network we are using in this class is a [Compositional Pattern-Producing Network](http://eplex.cs.ucf.edu/papers/stanley_gpem07.pdf) (CPPN). They are very simple neural networks that can be trained quickly, which is highly unusual for generative neural networks. So whats the catch? Well they can only learn to generate a single image (in the standard use-case). Still, they have a very unique aesthetic and would be a great topic for further investigation for your mini-project, especially with limited computational resources. \n",
    "\n",
    "CPPNs have a long and interesting history which predates the modern discourse on creativeAI and generativeAI. You [can watch this bonus lecture all about the subject from a previous year](https://ual.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=b3df426b-94fa-40b2-b774-af8e0115093e) if you want to know more about them. \n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import IPython.display\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Import utility functions from the file util.py in the src folder\n",
    "from src.util import get_normalised_coordinate_grid, make_training_gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define hyperparameters\n",
    "\n",
    "Here we define our hyperparameters, try to write a comment to define what each parameter does and why we need it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "device = 'cpu'\n",
    "#\n",
    "num_steps = 100000\n",
    "#\n",
    "batch_size = 100\n",
    "#\n",
    "learn_rate = 0.001\n",
    "#\n",
    "momentum = 0.9\n",
    "#\n",
    "num_channels = 3\n",
    "#\n",
    "image_shape = (128,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load target image\n",
    "\n",
    "Lets load in our target image for this training process. There are a few in the folder `media`, you can try different images here or load your own images into the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_im_path = '../media/colour_wheel.png'\n",
    "target_im = Image.open(target_im_path).convert('RGB')\n",
    "resized_im = target_im.resize(image_shape)\n",
    "resized_im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create coordinate grid\n",
    "\n",
    "Here we create a grid of normalised coordinates between the values -1,1 for every pixel in the 128x128 image, which is flattened into one long list of x and y coordinates. This gives us a matrix tensor which has a length of 16384 (128x128) and second dimension of length 2 (x and y coordinate values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xy_coordinates = get_normalised_coordinate_grid(image_shape) \n",
    "all_xy_coordinates = torch.tensor(all_xy_coordinates, device=device, dtype=torch.float32)\n",
    "print(f'coordinate grid shape: {all_xy_coordinates.shape}')\n",
    "print(f'coordinate grid data: \\n {all_xy_coordinates}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create pixel tensor\n",
    "\n",
    "Here we create our tensor containing our pixel values, normally this would be a 3-D tensor (width, height, channels), but we will flatten this to be a 2-D matrix tensor with the length 16384 (128x128 pixels) and second dimension of length 3 (red, green and blue pixel values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pixel_values = np.reshape(resized_im, [-1, num_channels]) / 255\n",
    "all_pixel_values = torch.tensor(all_pixel_values, device=device, dtype=torch.float32)\n",
    "print(f'image pixel tensor shape: {all_pixel_values.shape}')\n",
    "print(f'image pixel tensor data: \\n {all_pixel_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define neural network\n",
    "\n",
    "Here we define our CPPN neural network. Can you write comments for each line of code here?\n",
    "\n",
    "Every network you create in PyTorch will inherit from the [torch.nn.Module](https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/module.py) base class. This will mean that our network has all of the handy utility functions needed for us to be able to train it on our data. \n",
    "\n",
    "If you are unsure about anything you can consult [the pytorch neural network (torch.nn) reference](https://pytorch.org/docs/stable/nn.html) and [the W3 schools reference on python inheritance](https://www.w3schools.com/python/python_inheritance.asp). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPPN(nn.Module):\n",
    "    #\n",
    "    def __init__(self):\n",
    "      #\n",
    "      super(CPPN, self).__init__()\n",
    "      #\n",
    "      self.fc1 = nn.Linear(2, 16)\n",
    "      #\n",
    "      self.fc2 = nn.Linear(16, 32)\n",
    "      #\n",
    "      self.fc3 = nn.Linear(32, num_channels)     \n",
    "    \n",
    "    #\n",
    "    def forward(self, x):\n",
    "        #\n",
    "        x = self.fc1(x)\n",
    "        #\n",
    "        x = F.relu(x)\n",
    "        #\n",
    "        x = self.fc2(x)\n",
    "        #\n",
    "        x = F.relu(x)\n",
    "        #\n",
    "        x = self.fc3(x)\n",
    "        #\n",
    "        x = F.sigmoid(x)\n",
    "        #\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup core objects\n",
    "\n",
    "Here we setup our core objects, the model, the loss function and the optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cppn = CPPN()\n",
    "cppn.to(device)\n",
    "cppn.requires_grad_()\n",
    "\n",
    "optimiser = torch.optim.SGD(cppn.parameters(), lr=learn_rate, momentum=momentum)\n",
    "criterion = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training loop\n",
    "\n",
    "Here is our training loop for our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_coords = all_xy_coordinates.shape[0]\n",
    "coord_indexes = list(range(0, num_coords))\n",
    "losses = []\n",
    "img_list = []\n",
    "running_loss = 0.0\n",
    "\n",
    "for i in range(num_steps):\n",
    "    optimiser.zero_grad()\n",
    "    cppn.zero_grad()\n",
    "\n",
    "    # Sample a random batch of indexes from the list coord_indexes\n",
    "    batch_indexes = torch.tensor(np.array(random.sample(coord_indexes, batch_size)))\n",
    "    \n",
    "    # Get batch of respective xy_coordiantes\n",
    "    coordinates_batch = all_xy_coordinates[batch_indexes]\n",
    "    \n",
    "    # And respective pixel values \n",
    "    pixel_values_batch = all_pixel_values[batch_indexes]\n",
    "    \n",
    "    # Process data with model\n",
    "    approx_pixel_values = cppn(coordinates_batch)\n",
    "    \n",
    "    # Calculate and track loss function\n",
    "    loss = criterion(pixel_values_batch, approx_pixel_values)\n",
    "    running_loss += loss.item()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f'step {i}, loss {running_loss/1000:.3f}')\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            prediction = cppn(all_xy_coordinates)\n",
    "            prediction  = torch.swapaxes(prediction, 0, 1)\n",
    "            prediction = torch.reshape(prediction, (num_channels, image_shape[0], image_shape[1]))\n",
    "            if not os.path.exists('training_ims'):\n",
    "                os.makedirs('training_ims')\n",
    "            save_image(prediction, f'training_ims/im_{int(i/1000):06}.png')\n",
    "            img_list.append(prediction)\n",
    "            \n",
    "    #Update model\n",
    "    loss.backward()\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate whole image\n",
    "\n",
    "Here we are generating an entire image in one go. This may seem counter-intuitive but we are passing in our entire coordinate matrix, all 16384 cooridinates in one code. This means we are processing our data with a batch size of 16834 and **processing that data through 16384 copies of our neural network** all in one go! \n",
    "\n",
    "We can only do this because our network is so small and our modern computer have so much memory available to store and process all that data. As you will see in later weeks, we noramlly do inference with our networks with a batch size of 1, not 16384!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction = cppn(all_xy_coordinates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make training GIF\n",
    "\n",
    "Here lets make a gif of our training performance over time. This is a nice visual way of see how training occurs and whether there is a smooth convergence of our model or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_folder_path = 'training_ims'\n",
    "file_out = 'training_gif.gif'\n",
    "make_training_gif(im_folder_path=im_folder_path, im_ext='png', file_out=file_out)\n",
    "IPython.display.Image('training_gif.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interactive training animation\n",
    "\n",
    "We can also interactively look at training if we want to interactively zoom in on particular parts of the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "IPython.display.HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Side by side comparison\n",
    "\n",
    "Here is our final predicted image (left) side by side with our target image (right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping it from 1D to 2D\n",
    "reconstructed_img = np.reshape(prediction.cpu(), (image_shape[0], image_shape[1], num_channels)) #adding 3 because of RGB\n",
    "# scaling the values from [0,1] to [0, 255]\n",
    "reconstructed_img *= 255\n",
    "# converting the tensor into a numpy array, and cast the type into a uint8.\n",
    "reconstructed_img = reconstructed_img.numpy().astype(np.uint8)\n",
    "# looking at our creation next to the original!\n",
    "fig, axes_array = plt.subplots(1,2, figsize=(20,10))\n",
    "axes_array[0].imshow(resized_im)\n",
    "axes_array[1].imshow(reconstructed_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot loss\n",
    "\n",
    "Here is our training loss over time, what do you observe? Come back to this when you change the training hyperparameters to see if you see any difference here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Loss During Training\")\n",
    "plt.plot(losses,label=\"loss\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "**Task 1** Write comments for [the hyperparameters](#define-hyperparameters) and [code for defining the network](#define-neural-network). If there is code you don't understand and get stuck here don't get too bogged down, put a note raise it as a question at the end of the session. \n",
    "\n",
    "**Task 2:** Try [changing the hyperparameters](#define-hyperparameters) `learning_rate`, `momentum` and `batch_size` to see what effect they have. What is the highest learning rate you can use and still get a network that replicates the target image. What happens when you when make the momentum or batch size very low?\n",
    "\n",
    "**Task 3:** Try [loading in a different image](#load-target-image) and see how the network does there? Is the performance better or worse with a different image. Feel free to go and find your own image here and load it into the code.\n",
    "\n",
    "**Task 4:** Try [changing the architecture of the neural network](#define-neural-network). Add more layers, increase (or decrease) the number of units in each fully connected layer. Change the activation functions to [one of the other many available activation functions in pytorch](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions). How do all these things affect training. \n",
    "\n",
    "**Task 5:** Try [changing the optimiser](#setup-core-objects) from SGD (stochastic gradient descent) to one of the [many other optimisers in PyTorch](https://pytorch.org/docs/stable/optim.html). Are there any new hyperparameters that you can adjust in your optimiser? What effect does changing these hyperparameters have?\n",
    "\n",
    "#### Bonus tasks\n",
    "\n",
    "**Task A:** Can you revisit the code from Week 1 and finish writing comments for all of the lines of code now based on what you have learnt from this weeks lecture? Are there still any gaps?\n",
    "\n",
    "**Task B:** Can you rewrite this notebook to use a custom dataset class and a dataloader? Then can you rewrite the training loop to use epochs instead of iterations? See this PyTorch reference for more details: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
